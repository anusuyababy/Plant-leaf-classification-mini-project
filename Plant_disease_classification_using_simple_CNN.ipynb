{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MINI PROJECT\n"
      ],
      "metadata": {
        "id": "tmZL0TgnSDNS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxehY2ruS5Ez"
      },
      "source": [
        "### PLANT DISEASE IMAGE CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khCPkbZXTxyM"
      },
      "outputs": [],
      "source": [
        "#importing important libraries\n",
        "# Imports\n",
        "#not all used\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os.path\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from fastai import *\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.utils import img_to_array,  load_img\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from glob import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "2wYNbZguKkGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/cotton.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NxqNjL61UDr",
        "outputId": "63942b60-c7ad-4c6a-c621-289a3695dd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/cotton.zip\n",
            "replace cotton/test/diseased cotton leaf/dis_leaf (124).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path('/content/cotton/train') # make a directory \"data_dir\"/content/drive/MyDrive/DatesImages/DatesImages\n",
        "path = Path(data_dir)\n",
        "path.ls() #list all the folders of dates images.\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo0EizP_LVKD",
        "outputId": "ceb9f0bb-bf58-431d-bdff-822e0cb26c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/cotton/train')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir1 = pathlib.Path('/content/cotton/test') # make a directory \"data_dir\"/content/drive/MyDrive/DatesImages/DatesImages\n",
        "path1 = Path(data_dir1)\n",
        "path1.ls() #list all the folders of dates images.\n",
        "path1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8ZkZiGULmk9",
        "outputId": "b001f4f9-7637-44c3-ffb3-138fcaf12c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/cotton/test')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtjvqWIlKAqr",
        "outputId": "9408dbbc-dd20-4955-b942-f1edcdec53bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of all images in dataset: 122\n"
          ]
        }
      ],
      "source": [
        "def number_of_images_in_dataset(dataset):\n",
        "  images = list(dataset.glob(\"*/*.*\"))\n",
        "  return len(images)\n",
        "print(\"number of all images in dataset: {}\".format(number_of_images_in_dataset(data_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def number_of_images_in_dataset(dataset):\n",
        "  images = list(dataset.glob(\"*/*.*\"))\n",
        "  return len(images)\n",
        "print(\"number of all images in dataset: {}\".format(number_of_images_in_dataset(data_dir1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDqOU9o4LrWl",
        "outputId": "b5c312db-940b-4066-858f-78b162b2cbd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of all images in dataset: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "wObkXNM9ZWPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "GoVASPTaL0IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATASET LOADING"
      ],
      "metadata": {
        "id": "RmK8ztXMZZVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "train_dataset = ImageFolder(root=data_dir, transform=transform)\n",
        "test_dataset = ImageFolder(root=data_dir1, transform=transform)"
      ],
      "metadata": {
        "id": "-hk2BEF7OH3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for training\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "iYyicac4L0RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2"
      ],
      "metadata": {
        "id": "4Z5SlWWvOvly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BUILDING SIMPLE CNN MODEL"
      ],
      "metadata": {
        "id": "ziQrPCTeZgOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(16 * 128 * 128, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)  # Assuming 'num_classes' is the number of classes in your dataset\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 128 * 128)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()"
      ],
      "metadata": {
        "id": "WlCkbm3VNtpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "97oWWxo5Nts8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print training loss for this epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFy1KTO1NtxK",
        "outputId": "b2ab78e2-f794-4ea8-a5d1-9c964694cf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 5.323312946455992\n",
            "Epoch 2, Loss: 2.321087315427263\n",
            "Epoch 3, Loss: 0.9892421831056232\n",
            "Epoch 4, Loss: 1.3504804669951898\n",
            "Epoch 5, Loss: 1.1689230898351286\n",
            "Epoch 6, Loss: 0.8935395527676847\n",
            "Epoch 7, Loss: 0.8966442073302221\n",
            "Epoch 8, Loss: 0.8438396052510829\n",
            "Epoch 9, Loss: 0.8483576693566856\n",
            "Epoch 10, Loss: 0.8245837343894862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on validation data: {100 * correct / total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZG9efwwNt0y",
        "outputId": "1447e8b2-8666-473e-ea25-7e21c84dd474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on validation data: 90.9090909090909%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation"
      ],
      "metadata": {
        "id": "NwDjxpFHR6Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the same transform used for training and validation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the sample image and apply the same transform\n",
        "sample_image_path = '/content/cotton/test/fresh cotton leaf/d (61)_iaip - Copy.jpg'\n",
        "sample_image = Image.open(sample_image_path)\n",
        "sample_image = transform(sample_image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Load the trained model\n",
        "\n",
        "model = SimpleCNN()  # Create an instance of your model\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Make predictions on the sample image\n",
        "with torch.no_grad():\n",
        "    output = model(sample_image)\n",
        "\n",
        "# Assuming output is a tensor of shape (1, num_classes), where num_classes is the number of classes in your dataset\n",
        "# You can convert it to class probabilities using softmax\n",
        "import torch.nn.functional as F\n",
        "probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "# Get the predicted class label\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "\n",
        "# Define class names\n",
        "class_names = ['Disease', 'Normal']\n",
        "\n",
        "# Map the predicted class index to the class name\n",
        "predicted_class_name = class_names[predicted_class.item()]\n",
        "\n",
        "print(f\"The Predicted Class is : {predicted_class_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixzLV-fPL0ZN",
        "outputId": "7176e740-61e9-4e1c-f351-da0e507ac634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Predicted Class is : Normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the same transform used for training and validation\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load the sample image and apply the same transform\n",
        "sample_image_path = '/content/cotton/train/diseased cotton leaf/dis_leaf (24)_iaip.jpg'\n",
        "sample_image = Image.open(sample_image_path)\n",
        "sample_image = transform(sample_image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Load the trained model\n",
        "\n",
        "model = SimpleCNN()  # Create an instance of your model\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Make predictions on the sample image\n",
        "with torch.no_grad():\n",
        "    output = model(sample_image)\n",
        "\n",
        "# Assuming output is a tensor of shape (1, num_classes), where num_classes is the number of classes in your dataset\n",
        "# You can convert it to class probabilities using softmax\n",
        "import torch.nn.functional as F\n",
        "probabilities = F.softmax(output, dim=1)\n",
        "\n",
        "# Get the predicted class label\n",
        "_, predicted_class = torch.max(output, 1)\n",
        "\n",
        "# Define class names\n",
        "class_names = ['Disease', 'Normal']\n",
        "\n",
        "# Map the predicted class index to the class name\n",
        "predicted_class_name = class_names[predicted_class.item()]\n",
        "\n",
        "print(f\"The Predicted Class is : {predicted_class_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFwrH3NERymS",
        "outputId": "2ea68873-2126-4e8c-e169-c76e45b2e6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Predicted Class is : Disease\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}